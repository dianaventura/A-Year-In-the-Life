{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dianaventura/Defining-Success/blob/main/Keras_CNN_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6_tn40wr6r_"
      },
      "source": [
        "# CNN for IMDB Dataset\n",
        "\n",
        "In this notebook, we will demonstrate how to use keras to create, train and evaluate your own CNN algorithm on the imdb movie reviews dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qK0TlLEr6sF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import keras\n",
        "#Import sequence preprocessing methods\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "#Import the sequential architecture\n",
        "from keras.models import Sequential\n",
        "\n",
        "#Import the different layers needed to create a CNN\n",
        "#First, import your convolutional and max pooling layers\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, BatchNormalization, Flatten\n",
        "#Next, we import the dense (fully-connected) and embedding layers\n",
        "from keras.layers import Dense, Embedding\n",
        "#Finally, import your layer activations and regulizers - in this case, dropout and activation functions\n",
        "from keras.layers import Dropout, Activation\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Keras offers us the ability to import datasets directly into our code using a number of presets.\n",
        "#We can import all the datasets at once, but it consumes memory, so its better to import only the required one.\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LubwJEyr6sI"
      },
      "source": [
        "np.random.seed(1) #for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tu9ve2Ar6sJ"
      },
      "source": [
        "# Load the Dataset\n",
        "Unlike in some of our previous labs, Keras will handle the loading and splitting of the dataset for us in a single function call. This split is static (i.e. it does not change between runs) and is already stratified. If you want to design your own split, or use an evaluation methodology (such as cross-fold-validation or hold out) on your data, then additional work will be required.\n",
        "\n",
        "In this lab we will use the supplied Keras split, but you should keep the above in mind for your coursework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0v7GaE5r6sJ"
      },
      "source": [
        "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "\n",
        "\n",
        "That dictionary that is inverted when using load_data assumes the word indices start from 1.\n",
        "\n",
        "Keras uses index \"0\" to encode any unknown word; index 1 and 2 to mark the START of a sentence and any out of vocabulary (oov_char ) words such as stop words.\n",
        "Also oov_char replaces words that were cut out because of the num_words or skip_top limit.\n",
        "The parameter index_from parameter can be used to miss out these extra indexed word annotates.\n",
        "\n",
        "See [keras.datasets.imdb.load_data](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification ) method they explain what is being loaded.\n",
        "\n",
        "So basically the representation has already removed stop words and most of the puntuation and less frequent words are removed from text during preprocessing.\n",
        "\n",
        "It returns 2 tuples:\n",
        "- x_train, x_test: list of sequences, which are lists of indexes (integers). If the num_words argument was specified, the maximum possible index value is num_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen.\n",
        "- y_train, y_test: list of integer labels (1 or 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewbPI0RHr6sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0412f0e-b1a2-4609-80f1-4d5fbd9c1467"
      },
      "source": [
        "MAX_FEATURES=1000 # only use top occuring 1000 words\n",
        "INDEX_FROM =3   # word index offset\n",
        "\n",
        "# # modify the default parameters of np.load\n",
        "# np_load_old = np.load\n",
        "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "# #Load the dataset, and print the length of the train and test sets.\n",
        "# print('Loading data...')\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_FEATURES, index_from=INDEX_FROM)\n",
        "print('An example looks like this:')\n",
        "print(X_train[0])\n",
        "print('An example class label vector looks like this:', y_train[0])\n",
        "print(X_train.shape, 'train sequences')\n",
        "print(X_test.shape, 'test sequences')\n",
        "\n",
        "# # restore np.load for future normal usage\n",
        "# np.load = np_load_old"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n",
            "An example looks like this:\n",
            "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "An example class label vector looks like this: 1\n",
            "(25000,) train sequences\n",
            "(25000,) test sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g80PQ52or6sM"
      },
      "source": [
        "## How can we view the raw text?\n",
        "Since the load method provides the already indexed IDs of the word and not the raw words we can use word_to_id method to return the actual word from its index value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxRgnNAer6sM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04142c2b-8223-4e85-b54e-763ab3e6cbdc"
      },
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_to_id = keras.datasets.imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "\n",
        "# The first indices are reserved\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<OOV>\"] = 2 #out of vocabulary (less frequent)\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print('An example review reconstructed from the index:')\n",
        "print(' '.join(id_to_word[id] for id in X_train[0] ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 1s 1us/step\n",
            "An example review reconstructed from the index:\n",
            "<START> this film was just brilliant casting <OOV> <OOV> story direction <OOV> really <OOV> the part they played and you could just imagine being there robert <OOV> is an amazing actor and now the same being director <OOV> father came from the same <OOV> <OOV> as myself so i loved the fact there was a real <OOV> with this film the <OOV> <OOV> throughout the film were great it was just brilliant so much that i <OOV> the film as soon as it was released for <OOV> and would recommend it to everyone to watch and the <OOV> <OOV> was amazing really <OOV> at the end it was so sad and you know what they say if you <OOV> at a film it must have been good and this definitely was also <OOV> to the two little <OOV> that played the <OOV> of <OOV> and paul they were just brilliant children are often left out of the <OOV> <OOV> i think because the stars that play them all <OOV> up are such a big <OOV> for the whole film but these children are amazing and should be <OOV> for what they have done don't you think the whole story was so <OOV> because it was true and was <OOV> life after all that was <OOV> with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkj1EsdBr6sN"
      },
      "source": [
        "## Get Vocabulary and max length stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvHa9Pt1r6sN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc60019-db23-441b-e51c-6c2d3fb396c8"
      },
      "source": [
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "# summarize size\n",
        "print(\"Training data: \")\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: \n",
            "(50000,)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF0QULKlr6sN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5bcef4-634b-4c64-806d-1564314d9cc0"
      },
      "source": [
        "# Summarize number of classes\n",
        "print(\"Classes: \")\n",
        "print(np.unique(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: \n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BPXPGupr6sO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e9cfc3-90b0-49ee-8011-acd818eb47c1"
      },
      "source": [
        "# Summarize number of words\n",
        "print(\"Number of words: \")\n",
        "print(len(np.unique(np.hstack(X))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: \n",
            "998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42P9x2aqr6sO"
      },
      "source": [
        "Finally, we can get an idea of the average review length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpWC6tdfr6sO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "acae7d1b-0dfb-4526-e897-cf18c359ef75"
      },
      "source": [
        "# Summarize review length\n",
        "print(\"Review length: \")\n",
        "result = [len(x) for x in X]\n",
        "print(\"Mean %.2f words (%f) in a review\" % (np.mean(result), np.std(result)))\n",
        "# plot review length\n",
        "pyplot.boxplot(result)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review length: \n",
            "Mean 234.76 words (172.911495) in a review\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsVElEQVR4nO3df3RU9Z3/8dckmJAAE0DIrxJIJOVXE+WHXZJoKBFKwMAhCzmnCrS064+K4K5AkcbdVbEu2QN23fYouHStdE8LdYsR1qC4FBKJMqAbzWooILCkoPlFQWZCCAmZud8//OaWkUgDhMx8Js/HOXNk7uedmff0nGZeuffz+VyHZVmWAAAADBIW6AYAAACuFgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGCcXoFu4Ebx+XyqqalRv3795HA4At0OAADoBMuy1NjYqMTERIWFffV5lpANMDU1NUpKSgp0GwAA4BqcPHlSQ4YM+crxkA0w/fr1k/TF/wBOpzPA3QAAgM7weDxKSkqyv8e/SsgGmPbLRk6nkwADAIBh/tL0DybxAgAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGuaoAU1RUpG9+85vq16+fYmNjlZ+fr8OHD/vVTJ48WQ6Hw+/x0EMP+dWcOHFCeXl5io6OVmxsrFasWKG2tja/mrKyMo0fP16RkZFKTU3Vxo0br+0TAggpXq9XZWVl2rx5s8rKyuT1egPdEoAAuKoA8/bbb2vx4sXat2+fdu7cqYsXL2ratGlqamryq3vggQdUW1trP9asWWOPeb1e5eXlqbW1VXv37tWvfvUrbdy4UU888YRdc/z4ceXl5SknJ0eVlZV69NFHdf/99+utt966zo8LwGTFxcVKTU1VTk6O5s2bp5ycHKWmpqq4uDjQrQHobtZ1aGhosCRZb7/9tn3sW9/6lvV3f/d3X/kzb7zxhhUWFmbV1dXZx9avX285nU6rpaXFsizLeuyxx6xvfOMbfj/3ne98x8rNze10b26325Jkud3uTv8MgOD16quvWg6Hw5o1a5blcrmsxsZGy+VyWbNmzbIcDof16quvBrpFAF2gs9/f1zUHxu12S5IGDhzod/w3v/mNBg0apLS0NBUWFur8+fP2mMvlUnp6uuLi4uxjubm58ng8OnDggF0zdepUv9fMzc2Vy+W6nnYBGMrr9Wr58uWaOXOmtm7dqoyMDPXt21cZGRnaunWrZs6cqR/96EdcTgJ6kGu+maPP59Ojjz6qO+64Q2lpafbxefPmadiwYUpMTNRHH32klStX6vDhw/Yp3rq6Or/wIsl+XldXd8Uaj8ej5uZmRUVFXdZPS0uLWlpa7Ocej+daPxqAIFNeXq7q6mpt3rxZYWH+f3eFhYWpsLBQWVlZKi8v1+TJkwPTJIBudc0BZvHixaqqqtI777zjd/zBBx+0/52enq6EhARNmTJFx44d0/Dhw6+907+gqKhIq1atumGvDyBwamtrJcnvj6VLtR9vrwMQ+q7pEtKSJUtUUlKi0tJSDRky5Iq1EydOlCQdPXpUkhQfH6/6+nq/mvbn8fHxV6xxOp0dnn2RpMLCQrndbvtx8uTJq/9gAIJSQkKCJKmqqqrD8fbj7XUAQt9VBRjLsrRkyRK99tpr2r17t1JSUv7iz1RWVkr68y+WzMxMffzxx2poaLBrdu7cKafTqTFjxtg1u3bt8nudnTt3KjMz8yvfJzIyUk6n0+8BIDRkZ2crOTlZq1evls/n8xvz+XwqKipSSkqKsrOzA9QhgG53NTODFy1aZMXExFhlZWVWbW2t/Th//rxlWZZ19OhR6+mnn7b+53/+xzp+/Li1bds265ZbbrEmTZpkv0ZbW5uVlpZmTZs2zaqsrLR27NhhDR482CosLLRr/u///s+Kjo62VqxYYR08eNB64YUXrPDwcGvHjh1dPosZgBkuXYW0d+9ey+PxWHv37mUVEhBiOvv9fVUBRlKHj5dfftmyLMs6ceKENWnSJGvgwIFWZGSklZqaaq1YseKyJqqrq60ZM2ZYUVFR1qBBg6zly5dbFy9e9KspLS21xo4da0VERFi33HKL/R6dRYABQs+rr75qJScn+/3+SUlJIbwAIaSz398Oy7KsgJz6ucE8Ho9iYmLkdru5nASEEK/Xq/LyctXW1iohIUHZ2dkKDw8PdFsAukhnv7+veRUSAARCeHg4S6UBcDNHAABgHgIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMbhbtQAjOL1elVeXq7a2lolJCQoOztb4eHhgW4LQDfjDAwAYxQXFys1NVU5OTmaN2+ecnJylJqaquLi4kC3BqCbEWAAGKG4uFgFBQVKT0+Xy+VSY2OjXC6X0tPTVVBQQIgBehiHZVlWoJu4ETwej2JiYuR2u+V0OgPdDoDr4PV6lZqaqvT0dG3dulVhYX/+28vn8yk/P19VVVU6cuQIl5MAw3X2+5szMACCXnl5uaqrq/X444/7hRdJCgsLU2FhoY4fP67y8vIAdQiguxFgAAS92tpaSVJaWlqH4+3H2+sAhD4CDICgl5CQIEmqqqrqcLz9eHsdgNBHgAEQ9LKzs5WcnKzVq1fL5/P5jfl8PhUVFSklJUXZ2dkB6hBAdyPAAAh64eHh+ulPf6qSkhLl5+f7rULKz89XSUmJnn32WSbwAj0IG9kBMMKcOXO0ZcsWLV++XFlZWfbxlJQUbdmyRXPmzAlgdwC6G8uoARiFnXiB0NbZ72/OwAAwSnh4uCZPnhzoNgAEGHNgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIzTK9ANAMDV8Hq9Ki8vV21trRISEpSdna3w8PBAtwWgm3EGBoAxiouLlZqaqpycHM2bN085OTlKTU1VcXFxoFsD0M0IMACMUFxcrIKCAtXX1/sdr6+vV0FBASEG6GEIMACCntfr1aJFi2RZlqZMmSKXy6XGxka5XC5NmTJFlmVp0aJF8nq9gW4VQDchwAAIemVlZWpoaNCdd96pbdu2KSMjQ3379lVGRoa2bdumO+64Qw0NDSorKwt0qwC6CQEGQNBrDyarVq1SWJj/r62wsDA99dRTfnUAQh8BBgAAGIcAAyDoTZ48WZL05JNPyufz+Y35fD6tWrXKrw5A6CPAAAh6kydP1uDBg/XOO+9o9uzZfpN4Z8+erXfeeUexsbEEGKAHYSM7AEEvPDxcL774oubOnatdu3appKTEHouOjpYkrV+/ng3tgB6EMzAAjDBnzhy9+uqrio2N9TseGxurV199VXPmzAlQZwACwWFZlhXoJm4Ej8ejmJgYud1uOZ3OQLcDoItwKwEgtHX2+5tLSACMEh4ezlwXAFxCAgAA5iHAAAAA4xBgAACAcQgwAADAOFcVYIqKivTNb35T/fr1U2xsrPLz83X48GG/mgsXLmjx4sW6+eab1bdvX82dO1f19fV+NSdOnFBeXp6io6MVGxurFStWqK2tza+mrKxM48ePV2RkpFJTU7Vx48Zr+4QAACDkXFWAefvtt7V48WLt27dPO3fu1MWLFzVt2jQ1NTXZNUuXLtXrr7+u3/3ud3r77bdVU1Pjtz+D1+tVXl6eWltbtXfvXv3qV7/Sxo0b9cQTT9g1x48fV15ennJyclRZWalHH31U999/v956660u+MgAAMB017UPzKlTpxQbG6u3335bkyZNktvt1uDBg7Vp0yYVFBRIkg4dOqTRo0fL5XIpIyNDb775pmbOnKmamhrFxcVJkl588UWtXLlSp06dUkREhFauXKnt27erqqrKfq977rlHZ8+e1Y4dOzrVG/vAAABgns5+f1/XHBi32y1JGjhwoCSpoqJCFy9e1NSpU+2aUaNGaejQoXK5XJIkl8ul9PR0O7xIUm5urjwejw4cOGDXXPoa7TXtr9GRlpYWeTwevwcAAAhN1xxgfD6fHn30Ud1xxx1KS0uTJNXV1SkiIkL9+/f3q42Li1NdXZ1dc2l4aR9vH7tSjcfjUXNzc4f9FBUVKSYmxn4kJSVd60cDAABB7poDzOLFi1VVVaXf/va3XdnPNSssLJTb7bYfJ0+eDHRLAADgBrmmWwksWbJEJSUl2rNnj4YMGWIfj4+PV2trq86ePet3Fqa+vl7x8fF2zXvvvef3eu2rlC6t+fLKpfr6ejmdTkVFRXXYU2RkpCIjI6/l4wAAAMNc1RkYy7K0ZMkSvfbaa9q9e7dSUlL8xidMmKCbbrpJu3btso8dPnxYJ06cUGZmpiQpMzNTH3/8sRoaGuyanTt3yul0asyYMXbNpa/RXtP+GgAAoGe7qlVIDz/8sDZt2qRt27Zp5MiR9vGYmBj7zMiiRYv0xhtvaOPGjXI6nXrkkUckSXv37pX0xTLqsWPHKjExUWvWrFFdXZ2++93v6v7779fq1aslfbGMOi0tTYsXL9bf/M3faPfu3frbv/1bbd++Xbm5uZ3qlVVIAACYp9Pf39ZVkNTh4+WXX7ZrmpubrYcfftgaMGCAFR0dbf31X/+1VVtb6/c61dXV1owZM6yoqChr0KBB1vLly62LFy/61ZSWllpjx461IiIirFtuucXvPTrD7XZbkiy3231VPwcAAAKns9/f17UPTDDjDAwAAObpln1gAAAAAoEAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwzjXdzBEAAqW1tVXr1q3TsWPHNHz4cD388MOKiIgIdFsAuhkBBoAxHnvsMT333HNqa2uzj61YsUJLly7VmjVrAtgZgO7GJSQARnjssce0du1a3XzzzfrFL36h2tpa/eIXv9DNN9+stWvX6rHHHgt0iwC6EfdCAhD0Wltb1adPH91888369NNP1avXn08et7W1aciQITp9+rSampq4nAQYjnshAQgZ69atU1tbm5555hm/8CJJvXr10tNPP622tjatW7cuQB0C6G4EGABB79ixY5KkmTNndjjefry9DkDoI8AACHrDhw+XJJWUlHQ43n68vQ5A6GMODICgxxwYoOdgDgyAkBEREaGlS5eqvr5eQ4YM0YYNG1RTU6MNGzZoyJAhqq+v19KlSwkvQA/CPjAAjNC+z8tzzz2nH/7wh/bxXr16acWKFewDA/QwXEICYBR24gVCW2e/vwkwAAAgaDAHBgAAhCwCDAAAMA4BBgAAGIcAAwAAjMMyagBG8Xq9Ki8vV21trRISEpSdna3w8PBAtwWgm3EGBoAxiouLlZqaqpycHM2bN085OTlKTU1VcXFxoFsD0M0IMACMUFxcrIKCAqWnp8vlcqmxsVEul0vp6ekqKCggxAA9DPvAAAh6Xq9XqampSk9P19atWxUW9ue/vXw+n/Lz81VVVaUjR45wOQkwHPvAAAgZ5eXlqq6u1uOPP+4XXiQpLCxMhYWFOn78uMrLywPUIYDuxiReAEGvtrZWkpSWltbhJN60tDS/OgChjwADIOglJCRIkp5//nn927/9m6qrq+2x5ORkPfjgg351AEIfl5AABL3s7GwNHjxYhYWFSktL85vEm5aWpscff1yxsbHKzs4OdKsAugkBBoARHA6H/W/LsuwHgJ6JAAMg6JWXl6uhoUFFRUWqqqpSVlaWnE6nsrKydODAAa1evVoNDQ1M4gV6EAIMgKDXPjl3yZIlOnr0qEpLS7Vp0yaVlpbqyJEjWrJkiV8dgNDHJF4AQa99cm5VVZUyMjI0efJkv/Gqqiq/OgChjzMwAIJedna2kpOTtXr1avl8Pr8xn8+noqIipaSkMIkX6EEIMACCXnh4uH7605+qpKRE+fn5fquQ8vPzVVJSomeffZZdeIEehEtIAIwwZ84cbdmyRcuXL1dWVpZ9PCUlRVu2bNGcOXMC2B2A7sYZGABG+fLS6S9fUgLQMxBgABih/W7Ut956q98lpFtvvZW7UQM9EHejBhD0uBs10HNwN2oAIYO7UQP4MgIMgKB36d2oO8LdqIGehwADIOhdupFdR9jIDuh5CDAAgh4b2QH4MvaBARD02jeyKygo0OzZszV9+nRFRUWpublZO3bs0Pbt27VlyxYm8AI9CKuQABjjscce03PPPae2tjb7WK9evbR06VKtWbMmgJ0B6Cqd/f7mDAwAIxQXF+vZZ59VXl6eZsyYYZ+BefPNN/Xss88qIyOD3XiBHoQzMACCHvvAAD0H+8AACBmX7gNjWZbKysq0efNmlZWVybIs9oEBeiAuIQEIeu37uxw7dkz33nuvqqur7bHk5GQ988wzfnUAQh8BBkDQa9/fZcGCBZo5c6ZWrFjhNwdmwYIFfnUAQh9zYAAEvdbWVvXp00d9+vRR//799cc//tEeGzZsmM6ePaumpiY1NTUpIiIigJ0CuF7MgQEQMvbu3au2tja53W5duHBBGzZsUE1NjTZs2KALFy7I7Xarra1Ne/fuDXSrALoJl5AABL3PPvtMkjRu3Dh9/vnnevDBB+2xlJQUjRs3Th9++KFdByD0cQYGQNA7deqUJOnhhx/W0aNHVVpaqk2bNqm0tFRHjhzRQw895FcHIPRddYDZs2ePZs2apcTERDkcDm3dutVv/Pvf/74cDoffY/r06X41Z86c0fz58+V0OtW/f3/dd999OnfunF/NRx99pOzsbPXu3VtJSUnssgn0YIMHD5b0xWZ2DodDkydP1r333qvJkyf7/R5qrwMQ+q46wDQ1Nem2227TCy+88JU106dPV21trf3YvHmz3/j8+fN14MAB7dy5UyUlJdqzZ4/fKWGPx6Np06Zp2LBhqqio0Nq1a/XUU09pw4YNV9sugBDwta99TZL05ptvKj8/Xy6XS42NjXK5XMrPz9ebb77pVwcg9F3XKiSHw6HXXntN+fn59rHvf//7Onv27GVnZtodPHhQY8aM0fvvv6/bb79dkrRjxw7dfffd+vTTT5WYmKj169fr7//+71VXV2evKPjxj3+srVu36tChQ53qjVVIQOho34l30KBBOnXqlN8qpOTkZA0aNEinT59mJ14gBAR0FVJZWZliY2M1cuRILVq0SKdPn7bHXC6X+vfvb4cXSZo6darCwsK0f/9+u2bSpEl+yyFzc3N1+PBhff755x2+Z0tLizwej98DQGhovxt1RUWF0tPT9fzzz+ull17S888/r7S0NFVUVOjZZ58lvAA9SJevQpo+fbrmzJmjlJQUHTt2TI8//rhmzJghl8ul8PBw1dXVKTY21r+JXr00cOBA1dXVSZLq6uqUkpLiVxMXF2ePDRgw4LL3LSoq0qpVq7r64wAIEnPmzNGWLVu0fPlylZSU2MdTUlK0ZcsWbuQI9DBdHmDuuece+9/p6em69dZbNXz4cJWVlWnKlCld/Xa2wsJCLVu2zH7u8XiUlJR0w94PQPebM2eOZs+erfLyctXW1iohIUHZ2dmceQF6oBu+D8wtt9yiQYMG6ejRo5oyZYri4+PV0NDgV9PW1qYzZ84oPj5ekhQfH6/6+nq/mvbn7TVfFhkZqcjIyBvwCQAEk/DwcE2ePDnQbQAIsBu+D8ynn36q06dP2/coyczM1NmzZ1VRUWHX7N69Wz6fTxMnTrRr9uzZo4sXL9o1O3fu1MiRIzu8fAQAAHqWqw4w586dU2VlpSorKyVJx48fV2VlpU6cOKFz585pxYoV2rdvn6qrq7Vr1y7Nnj1bqampys3NlSSNHj1a06dP1wMPPKD33ntP7777rpYsWaJ77rlHiYmJkqR58+YpIiJC9913nw4cOKBXXnlFP/vZz/wuEQEAgJ7rqpdRl5WVKScn57LjCxcu1Pr165Wfn68PP/xQZ8+eVWJioqZNm6af/OQn9iRc6YuN7JYsWaLXX39dYWFhmjt3rn7+85+rb9++ds1HH32kxYsX6/3339egQYP0yCOPaOXKlZ3uk2XUAACYp7Pf39yNGgAABI3Ofn9zM0cARvF6vaxCAsDNHAGYo7i4WKmpqcrJydG8efOUk5Oj1NRUFRcXB7o1AN2MAAPACMXFxSooKOhwi4WCggJCDNDDEGAABD2v16tFixbJsix9edpe+7FFixbJ6/UGqEMA3Y0AAyDolZWV2RtgTp061e9u1FOnTpUkNTQ0qKysLIBdAuhOBBgAQW/37t2Svtjksri4WBcuXNDrr7+uCxcuqLi42N4Es70OQOhjFRKAoHfixAlJ0pgxYzRixAhVV1fbY8nJybrrrru0f/9+uw5A6OMMDICgN3ToUEnSSy+9pLS0NL9LSGlpafrlL3/pVwcg9BFgAAS9b33rW/a/2yftXvroqA5AaOMSEoCgd+lGdbt379b27dvt59HR0R3WAQhtnIEBEPTaVyBJ6nAZdUd1AEIbAQZA0EtISJAkzZ8/XxcvXvQba21t1bx58/zqAIQ+buYIIOh5vV4lJiaqoaFBeXl5uvvuuxUVFaXm5ma98cYb2r59u2JjY1VTU8NlJMBwnf3+5gwMACO0/63lcDg0btw4FRQUaNy4cXI4HAHuDEAgEGAABL3y8nKdOnVKRUVFqqqqUlZWlpxOp7KysnTgwAGtXr1aDQ0NKi8vD3SrALoJAQZA0KutrZUkJSUlXTaJ1+fz2fu/tNcBCH0sowYQ9Non5y5YsEC9e/f2G6uvr9eCBQv86gCEPs7AAAh6WVlZCgu78q+rsLAwZWVldVNHAAKNAAMg6JWXl8vn80mSYmJitGHDBtXU1GjDhg2KiYmR9MWlJObAAD0Hl5AABL32u0yPGDFCra2tevDBB+2xlJQUjRgxQp988ol2796tKVOmBKpNAN2IMzAAgl77XaYfeeQRHT16VKWlpdq0aZNKS0t15MgRLV682K8OQOjjDAyAoNe+ymjTpk16+OGHNXnyZHvM5/Np8+bNfnUAQh9nYAAEvbvuukuS5HK5NHv2bLlcLjU2NtrP9+3b51cHIPRxKwEAQe/SWwlERkaqpaXFHuvdu7cuXLjArQSAEMGtBACEjPDwcK1fv17SFzdvvFR7mFm/fj3hBehBCDAAjOFwOC7byK53797cDwnogbiEBCDoeb1epaamatCgQaqvr9fJkyftsaSkJMXFxen06dM6cuQIZ2EAw3X2+5tVSACCXnl5uaqrq1VdXa2oqCi/sT/96U92oCkvL/dboQQgdHEJCUDQ++yzz+x/T5kyxW8V0qUb111aByC0cQYGQNCrq6uTJN16663atm2bfV+kjIwMbdu2TePGjdNHH31k1wEIfZyBARD0zpw5I0nq06dPh+PR0dF+dQBCHwEGQNBrP+Oyb98+5efn+11Cys/P1/79+/3qAIQ+/t8OIOi1T8wdOXKkPv74Y2VlZcnpdCorK0tVVVUaOXKkXx2A0McyagBB79KdeKdPn66mpiadPn1aN998s/r06aMdO3awEy8QIlhGDSBktO/EO3fuXO3YsaPDGnbiBXoWLiEBMMJ//Md/XNc4gNDCJSQAQa+5udleaXT33XcrLy9PUVFRam5u1vbt2/XGG29Iks6fP3/ZRncAzMIlJAAhY/ny5ZKk1NRUvf76636rjR566CGNGDFCx44d0/Lly7Vu3bpAtQmgG3EJCUDQe//99yVJRUVFly2VDgsL0z/90z/51QEIfQQYAEFvwIABkiSXy9XhePvx9joAoY85MACC3ltvvaXp06erV69e+vzzz/Xv//7vOnbsmIYPH677779fAwYMUFtbm3bs2KHc3NxAtwvgOnT2+5sAAyDoeb1eOZ1OnT9//itroqOj5fF4WEoNGK6z399cQgIQ9MLDw/Xtb3/7ijXf/va3CS9AD8IZGABBr7W1VX369FFYWJhaW1svG4+IiJDP51NTU5MiIiIC0CGArsIZGAAhY926dWpra+swvEhfBJy2tjaWUAM9CAEGQNA7fPhwl9YBMB8BBkDQ+/TTT7u0DoD52IkXQNA7duyY/e/p06dr5MiRam5uVlRUlA4fPmzf4PHSOgChjUm8AIJeTEyMPB7PX6xzOp1yu93d0BGAG4VJvABCxk033dSldQDMR4ABEPTS09O7tA6A+QgwAILe8OHDu7QOgPkIMACC3sGDB7u0DoD5CDAAgt6ZM2e6tA6A+QgwAILeH//4xy6tA2A+AgyAoNfc3NyldQDMd9UBZs+ePZo1a5YSExPlcDi0detWv3HLsvTEE08oISFBUVFRmjp1qo4cOeJXc+bMGc2fP19Op1P9+/fXfffdp3PnzvnVfPTRR8rOzlbv3r2VlJSkNWvWXP2nAwAAIemqA0xTU5Nuu+02vfDCCx2Or1mzRj//+c/14osvav/+/erTp49yc3N14cIFu2b+/Pk6cOCAdu7cqZKSEu3Zs0cPPvigPe7xeDRt2jQNGzZMFRUVWrt2rZ566ilt2LDhGj4iAAAIOdZ1kGS99tpr9nOfz2fFx8dba9eutY+dPXvWioyMtDZv3mxZlmX94Q9/sCRZ77//vl3z5ptvWg6Hw/rss88sy7KsdevWWQMGDLBaWlrsmpUrV1ojR47sdG9ut9uSZLnd7mv9eACChKROPwCYrbPf3106B+b48eOqq6vT1KlT7WMxMTGaOHGiXC6XJMnlcql///66/fbb7ZqpU6cqLCxM+/fvt2smTZqkiIgIuyY3N1eHDx/W559/3uF7t7S0yOPx+D0AAEBo6tIAU1dXJ0mKi4vzOx4XF2eP1dXVKTY21m+8V69eGjhwoF9NR69x6Xt8WVFRkWJiYuxHUlLS9X8gAAAQlEJmFVJhYaHcbrf9OHnyZKBbAgAAN0iXBpj4+HhJUn19vd/x+vp6eyw+Pl4NDQ1+421tbTpz5oxfTUevcel7fFlkZKScTqffAwAAhKYuDTApKSmKj4/Xrl277GMej0f79+9XZmamJCkzM1Nnz55VRUWFXbN79275fD5NnDjRrtmzZ48uXrxo1+zcuVMjR47UgAEDurJlAABgoKsOMOfOnVNlZaUqKyslfTFxt7KyUidOnJDD4dCjjz6qZ555Rv/1X/+ljz/+WN/73veUmJio/Px8SdLo0aM1ffp0PfDAA3rvvff07rvvasmSJbrnnnuUmJgoSZo3b54iIiJ033336cCBA3rllVf0s5/9TMuWLeuyDw4AAAx2tcubSktLO1y6uHDhQsuyvlhK/Y//+I9WXFycFRkZaU2ZMsU6fPiw32ucPn3auvfee62+fftaTqfT+sEPfmA1Njb61fzv//6vdeedd1qRkZHW1772Neuf//mfr6pPllEDoaOj3zlf9QBgts5+fzssy7K6OzR1B4/Ho5iYGLndbubDAIZzOBydrg3RX2lAj9HZ7++QWYUEAAB6DgIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGCcXl39gk899ZRWrVrld2zkyJE6dOiQJOnChQtavny5fvvb36qlpUW5ublat26d4uLi7PoTJ05o0aJFKi0tVd++fbVw4UIVFRWpV68ubxdANzl//rz9e+BG+uCDD676Z0aNGqXo6Ogb0A2AG+WGJIJvfOMb+v3vf//nN7kkeCxdulTbt2/X7373O8XExGjJkiWaM2eO3n33XUmS1+tVXl6e4uPjtXfvXtXW1up73/uebrrpJq1evfpGtAugGxw6dEgTJky44e9zLe9RUVGh8ePH34BuANwoNyTA9OrVS/Hx8Zcdd7vdeumll7Rp0ybdddddkqSXX35Zo0eP1r59+5SRkaH//u//1h/+8Af9/ve/V1xcnMaOHauf/OQnWrlypZ566ilFRETciJYB3GCjRo1SRUXFNf3s1YSSa3mPUaNGXfXPAAisGxJgjhw5osTERPXu3VuZmZkqKirS0KFDVVFRoYsXL2rq1Kl27ahRozR06FC5XC5lZGTI5XIpPT3d75JSbm6uFi1apAMHDmjcuHEdvmdLS4taWlrs5x6P50Z8NADXKDo6+prPcuTl5Wn79u2dquNMCtAzdPkk3okTJ2rjxo3asWOH1q9fr+PHjys7O1uNjY2qq6tTRESE+vfv7/czcXFxqqurkyTV1dX5hZf28faxr1JUVKSYmBj7kZSU1LUfDEDAlJSUdGkdAPN1+RmYGTNm2P++9dZbNXHiRA0bNkz/+Z//qaioqK5+O1thYaGWLVtmP/d4PIQYIIRYliWHw3HFcQA9xw1fRt2/f3+NGDFCR48eVXx8vFpbW3X27Fm/mvr6envOTHx8vOrr6y8bbx/7KpGRkXI6nX4PAKHFsizl5eX5HcvLyyO8AD3QDQ8w586d07Fjx5SQkKAJEybopptu0q5du+zxw4cP68SJE8rMzJQkZWZm6uOPP1ZDQ4Nds3PnTjmdTo0ZM+ZGtwsgyJWUlNgTdSsqKrhsBPRQXX4J6Uc/+pFmzZqlYcOGqaamRk8++aTCw8N17733KiYmRvfdd5+WLVumgQMHyul06pFHHlFmZqYyMjIkSdOmTdOYMWP03e9+V2vWrFFdXZ3+4R/+QYsXL1ZkZGRXtwsAAAzU5QHm008/1b333qvTp09r8ODBuvPOO7Vv3z4NHjxYkvTcc88pLCxMc+fO9dvIrl14eLhKSkq0aNEiZWZmqk+fPlq4cKGefvrprm4VAAAYymGF6MVjj8ejmJgYud1u5sMAIeaDDz7QhAkT2IAOCEGd/f7mXkgAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACM0yvQDQAIfkeOHFFjY2Og27AdPHjQ77/Bol+/fvr6178e6DaAHoEAA+CKjhw5ohEjRgS6jQ4tWLAg0C1c5pNPPiHEAN2AAAPgitrPvPz617/W6NGjA9zNF5qbm1VdXa3k5GRFRUUFuh1JX5wNWrBgQVCdqQJCGQEGQKeMHj1a48ePD3QbtjvuuCPQLQAIICbxAgAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA43AsJwBU52i5oXHyYos5+ItXwN89XiTr7icbFh8nRdiHQrQA9AgEGwBX1PndCH/ywr7Tnh9KeQHcTvEZL+uCHfXXw3AlJWYFuBwh5BBgAV3Sh71CN/7dz+s1vfqPRo0YFup2gdfDQIc2fP18v3T000K0APQIBBsAVWb1668M6n5r7j5ASxwa6naDVXOfTh3U+Wb16B7oVoEfggjYAADAOAQYAABiHS0gAruj8+fOSpA8++CDAnfxZc3OzqqurlZycrKioqEC3I0k6ePBgoFsAehQCDIArOnTokCTpgQceCHAnZujXr1+gWwB6BAIMgCvKz8+XJI0aNUrR0dGBbeb/O3jwoBYsWKBf//rXGj16dKDbsfXr109f//rXA90G0CMQYABc0aBBg3T//fcHuo0OjR49WuPHjw90GwACgEm8AADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGCcoA4wL7zwgpKTk9W7d29NnDhR7733XqBbAgAAQSBoA8wrr7yiZcuW6cknn9QHH3yg2267Tbm5uWpoaAh0awAAIMCC9maO//Iv/6IHHnhAP/jBDyRJL774orZv365f/vKX+vGPfxzg7gBcrfPnz+vQoUNd8loHDx70++/1CqY7bQPonKAMMK2traqoqFBhYaF9LCwsTFOnTpXL5erwZ1paWtTS0mI/93g8N7xPAJ136NAhTZgwoUtfc8GCBV3yOhUVFdzVGjBMUAaYP/3pT/J6vYqLi/M7HhcX95V/wRUVFWnVqlXd0R6AazBq1ChVVFR0yWs1NzerurpaycnJioqKuu7XGzVqVBd0BaA7BWWAuRaFhYVatmyZ/dzj8SgpKSmAHQG4VHR0dJee5bjjjju67LUAmCcoA8ygQYMUHh6u+vp6v+P19fWKj4/v8GciIyMVGRnZHe0BAIAAC8pVSBEREZowYYJ27dplH/P5fNq1a5cyMzMD2BkAAAgGQXkGRpKWLVumhQsX6vbbb9df/dVf6V//9V/V1NRkr0oCAAA9V9AGmO985zs6deqUnnjiCdXV1Wns2LHasWPHZRN7AQBAz+OwLMsKdBM3gsfjUUxMjNxut5xOZ6DbAQAAndDZ7++gnAMDAABwJQQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxgnYn3uvVvj+fx+MJcCcAAKCz2r+3/9I+uyEbYBobGyVJSUlJAe4EAABcrcbGRsXExHzleMjeSsDn86mmpkb9+vWTw+EIdDsAupDH41FSUpJOnjzJrUKAEGNZlhobG5WYmKiwsK+e6RKyAQZA6OJeZwCYxAsAAIxDgAEAAMYhwAAwTmRkpJ588klFRkYGuhUAAcIcGAAAYBzOwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDABj7NmzR7NmzVJiYqIcDoe2bt0a6JYABAgBBoAxmpqadNttt+mFF14IdCsAAixkb+YIIPTMmDFDM2bMCHQbAIIAZ2AAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHVUgAjHHu3DkdPXrUfn78+HFVVlZq4MCBGjp0aAA7A9DduBs1AGOUlZUpJyfnsuMLFy7Uxo0bu78hAAFDgAEAAMZhDgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxvl/VFdk0arehGwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbQ-c7bFr6sO"
      },
      "source": [
        "Looking at a box and whisker plot for the review lengths in words, we can probably see an exponential distribution that we can probably cover the mass of the distribution with a clipped length of 400 to 500 words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDvCA7hr6sO"
      },
      "source": [
        "## Pad the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UqVIjH3r6sP"
      },
      "source": [
        "\n",
        "After we have loaded the dataset, we need to pad it so that all examples are the same size for submission to the network. We can do that by using a function in Keras.\n",
        "Here we will bound reviews at maxlen words, truncating longer reviews and zero-padding shorter reviews.\n",
        "\n",
        "Note that padding will not be needed if you are using sparse representatsions such as from a tfidf vectoriser.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQOeHslpr6sP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124cb686-a159-4590-8362-fc6b8c82549b"
      },
      "source": [
        "# set parameters\n",
        "maxlen = 400\n",
        "\n",
        "#Find the max example size and ensure that all other examples are padded to this size\n",
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "#Print the data's shape (i.e. the size of each dimension) to check this is done correctly.\n",
        "print('x_train shape:', X_train.shape)\n",
        "print('x_test shape:', X_test.shape)\n",
        "print('After padding the example looks:')\n",
        "print(X_train[0])\n",
        "\n",
        "num_classes = len(np.unique(y))\n",
        "# convert class labels to binary class one-hot-encoded vectors\n",
        "y_train_ohe = to_categorical(y_train, num_classes)\n",
        "y_test_ohe = to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 400)\n",
            "x_test shape: (25000, 400)\n",
            "After padding the example looks:\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   1  14  22  16  43 530 973   2   2  65 458   2  66   2   4 173\n",
            "  36 256   5  25 100  43 838 112  50 670   2   9  35 480 284   5 150   4\n",
            " 172 112 167   2 336 385  39   4 172   2   2  17 546  38  13 447   4 192\n",
            "  50  16   6 147   2  19  14  22   4   2   2 469   4  22  71  87  12  16\n",
            "  43 530  38  76  15  13   2   4  22  17 515  17  12  16 626  18   2   5\n",
            "  62 386  12   8 316   8 106   5   4   2   2  16 480  66   2  33   4 130\n",
            "  12  16  38 619   5  25 124  51  36 135  48  25   2  33   6  22  12 215\n",
            "  28  77  52   5  14 407  16  82   2   8   4 107 117   2  15 256   4   2\n",
            "   7   2   5 723  36  71  43 530 476  26 400 317  46   7   4   2   2  13\n",
            " 104  88   4 381  15 297  98  32   2  56  26 141   6 194   2  18   4 226\n",
            "  22  21 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25 104\n",
            "   4 226  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16\n",
            "   2  19 178  32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veTX4e1Fr6sP"
      },
      "source": [
        "Recreate the textual content after the padding has been added. You can use the word_to_id method as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WduGiiR9o1I7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871059da-b3dd-4071-a6e2-3647962b9cff"
      },
      "source": [
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in X_train[0] ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film was just brilliant casting <OOV> <OOV> story direction <OOV> really <OOV> the part they played and you could just imagine being there robert <OOV> is an amazing actor and now the same being director <OOV> father came from the same <OOV> <OOV> as myself so i loved the fact there was a real <OOV> with this film the <OOV> <OOV> throughout the film were great it was just brilliant so much that i <OOV> the film as soon as it was released for <OOV> and would recommend it to everyone to watch and the <OOV> <OOV> was amazing really <OOV> at the end it was so sad and you know what they say if you <OOV> at a film it must have been good and this definitely was also <OOV> to the two little <OOV> that played the <OOV> of <OOV> and paul they were just brilliant children are often left out of the <OOV> <OOV> i think because the stars that play them all <OOV> up are such a big <OOV> for the whole film but these children are amazing and should be <OOV> for what they have done don't you think the whole story was so <OOV> because it was true and was <OOV> life after all that was <OOV> with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dth38DR-r6sQ"
      },
      "source": [
        "# Creating the CNN Architecture\n",
        "Now lets create a CNN model. We need to identify several things at this stage:\n",
        "\n",
        "(1) The parameters of our model (i.e. the hyperparameters of the CNN)\n",
        "\n",
        "(2) The architecture of our model (i.e. the layers of the CNN)\n",
        "\n",
        "Firstly lets define the architecture of the model by creating the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJxwXsCzr6sQ"
      },
      "source": [
        "def create_model(max_features = 1000, embedding_dims = 50,  maxlen = 400):\n",
        "    #Firstly, instantiate a sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    #We can now add layers to this model.\n",
        "    #Layers are added in the order of Input -> Output (meaning that they are sequential hence the name).\n",
        "    #Start off with an efficient embedding layer which maps vocab indices into embedding_dims dimensions\n",
        "    #max_features is the same as vocabulary size defined above\n",
        "    model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "\n",
        "    #Add a Convolution1D, which will learn filters\n",
        "    model.add(Conv1D(256, 3, padding='valid', activation='relu', strides=1))\n",
        "\n",
        "    #Add Batch Normalisation to avoid over-fitting\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    #The use max pooling:\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "\n",
        "    #Add a hidden layer:\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    #With batch normalisation\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    #Project onto a two unit output layer; two for the pos, neg class set\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdlxe4ctr6sQ"
      },
      "source": [
        "### Create the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ftfLEGr6sQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82078770-1279-4a28-a162-d5a2084e4fe2"
      },
      "source": [
        "#Firstly, lets create our model by calling the function we just made\n",
        "cnn = create_model()\n",
        "cnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 400, 50)           50000     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 398, 256)          38656     \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 398, 256)          1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 123346 (481.82 KB)\n",
            "Trainable params: 122578 (478.82 KB)\n",
            "Non-trainable params: 768 (3.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onvrKHGQr6sQ"
      },
      "source": [
        "### Training the model\n",
        "Now that we have a function to create our model, we can apply the training hyperparameters to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgiVN996r6sR"
      },
      "source": [
        "#Now lets state the hyperparameters as variables\n",
        "batch_size = 128\n",
        "epochs = 10 # try 1 epoch at first\n",
        "\n",
        "#Loss dictates what function we should use to determine the error\n",
        "loss = 'binary_crossentropy'\n",
        "\n",
        "#The optimizer is the function which controls how that error is applied to the network\n",
        "optimizer = 'adam'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l427jI0Hr6sR"
      },
      "source": [
        "#Now we can compile and train our model\n",
        "cnn.compile(loss = loss,  optimizer = optimizer, metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meVPEWWahJr8"
      },
      "source": [
        "#The 'fit' function will train on your data, and evaluate the accuracy of its performance on the test set at each epoch\n",
        "#First call the function and specify training data\n",
        "history = cnn.fit(X_train, y_train_ohe,\n",
        "        #The identify the batch size\n",
        "        batch_size = batch_size,\n",
        "        #Specify your epochs\n",
        "        epochs = epochs,\n",
        "        #And identify your evaluation data (remember to put brackets around both x_test and y_test)\n",
        "        validation_data = (X_test, y_test_ohe))\n",
        "\n",
        "#note in general evaluation dataset should not be the test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygtEbIxGr6sR"
      },
      "source": [
        "### Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "httjOJzSr6sR"
      },
      "source": [
        "score = cnn.evaluate(X_test, y_test_ohe, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(score[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1-BalLJr6sT"
      },
      "source": [
        "Lets create helper function to visualize the loss and accuracy for the training and testing data based on the History callback. This callback, which is automatically applied to each Keras model, records the loss and additional metrics that can be added in the .fit() method. In this case, we are only interested in the accuracy. This helper function employs the matplotlib plotting library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4rbJnRXr6sV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTwFDSlSr6sV"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X37NZdoEr6sY"
      },
      "source": [
        "The above demonstrates clearly that there is over fitting since training accuracy is significantly better than test / validation.\n",
        "The sweet spot is around 1 or 2 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHi7WfI1r6sY"
      },
      "source": [
        "# Creating and ANN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max_features = 1000, embedding_dims = 50,  maxlen = 400\n",
        "model = Sequential()\n",
        "model.add (Embedding(1000, 50, input_length=400))\n",
        "model.add (Flatten())\n",
        "model.add (Dense (units = 1024, activation='relu'))\n",
        "model.add (Dense ( units = 128, activation='relu'))\n",
        "model.add (Dense ( units = 2, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "DxdPZ79lYDYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "ann_history = model.fit(X_train, y_train_ohe, batch_size=128, epochs=10, validation_data = (X_test, y_test_ohe))\n",
        "score = model.evaluate(X_test, y_test_ohe)\n",
        "print('Total loss on the test set', score[0])\n",
        "print('Total accuracy on the test set', score[1])"
      ],
      "metadata": {
        "id": "s077DOLOYRIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(ann_history)"
      ],
      "metadata": {
        "id": "9j2OiM4OYWPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqyuY6KYr6sZ"
      },
      "source": [
        "# Preparing text from CSV files for a Keras model input\n",
        "\n",
        "Often we may want to read our data from a CSV and if it is text content them we want to santise that text for instance using regular expressions or stemming etc (as we did last week).\n",
        "Once that is done we need to convert it into a index vector that can be input.\n",
        "\n",
        "For this we make use of the keras tokeniser  and texts_to_sequence encoder.\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "We look at how to do this next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPIKLuarr6sZ"
      },
      "source": [
        "import os\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjsKBYKdr6sZ"
      },
      "source": [
        "# remove the non-word chars '[\\W]+'\n",
        "# append the emoticons to end\n",
        "# convert all to lowercase\n",
        "# remove nose char for consistency\n",
        "# remove all html markup\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # findall the emoticons\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
        "            ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0RpeehNkO4r"
      },
      "source": [
        "# Read data from file\n",
        "data_file = \"https://raw.githubusercontent.com/nirmalie/CM4107/main/movie_data_cat.csv\"\n",
        "df = pd.read_csv(data_file,  encoding='utf-8')\n",
        "\n",
        "#or upload file here and read\n",
        "# df = pd.read_csv('movie_data_cat.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H24sqNN0lYFv"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMQdFwhXlaXz"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItWtLC6Zr6sZ"
      },
      "source": [
        "class_mapping = {label:idx for idx,label in enumerate(np.unique(df['sentiment']))}\n",
        "\n",
        "#use the mapping dictionary to transform the class labels into integers\n",
        "df['sentiment'] = df['sentiment'].map(class_mapping)\n",
        "\n",
        "df['review'] = df['review'].apply(preprocessor)\n",
        "\n",
        "#make a train test split, we are using a smaller split here, but try the original split as well to compare performance later\n",
        "X_text_train = df.loc[:2500, 'review'].values\n",
        "Y_train = df.loc[:2500, 'sentiment'].values\n",
        "X_text_test = df.loc[2500:5000, 'review'].values\n",
        "Y_test = df.loc[2500:5000, 'sentiment'].values\n",
        "\n",
        "# convert class labels to binary class one-hot-encoded vectors\n",
        "y_train_ohe = to_categorical(Y_train, num_classes)\n",
        "y_test_ohe = to_categorical(Y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaRDv_Ugr6sa"
      },
      "source": [
        "print(X_text_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEVTzuH5r6sa"
      },
      "source": [
        "### How do we convert the text to a format suited to keras CNN input?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xXUqvSIgr6sa"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# create the tokenizer\n",
        "tokeniser = Tokenizer(num_words=5000)\n",
        "\n",
        "# fit the tokenizer on the documents\n",
        "tokeniser.fit_on_texts(X_text_train)\n",
        "\n",
        "# summarize what was learned\n",
        "print(tokeniser.word_counts)\n",
        "print(tokeniser.word_index)\n",
        "print(tokeniser.word_docs)\n",
        "print('Num of training documents', tokeniser.document_count)\n",
        "\n",
        "\n",
        "# integer encode documents\n",
        "X_encoded_train = tokeniser.texts_to_sequences(X_text_train)\n",
        "train_vocab_size = len(tokeniser.word_index) + 1 # Adding 1 because of reserved 0 index\n",
        "\n",
        "X_encoded_test = tokeniser.texts_to_sequences(X_text_test)\n",
        "test_vocab_size = len(tokeniser.word_index) + 1 # Adding 1 because of reserved 0 index\n",
        "\n",
        "print ('train vocabulary size:', train_vocab_size)\n",
        "print ('test vocabulary size:', test_vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMX9yrEzn0gE"
      },
      "source": [
        "maxlen = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL38KtPar6sa"
      },
      "source": [
        "# NOTE: padding is not needed when using tfidf sparce representations\n",
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(X_encoded_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_encoded_test, maxlen=maxlen)\n",
        "\n",
        "#Print the data's shape (i.e. the size of each dimension) to check this is done correctly.\n",
        "print('x_train shape:', X_train.shape)\n",
        "print('x_test shape:', X_test.shape)\n",
        "print('After padding the example looks:')\n",
        "print(X_train[0])\n",
        "print(len(X_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj85fw3Cr6sb"
      },
      "source": [
        "Now that the input is in the same format as we had when we loaded it directly from Keras; we can now use this as input into a CNN.\n",
        "This means that you can now compare the models on text classification with the CNN model using similar text pre-processing steps.\n",
        "\n",
        "Use the X_train and X_test together with the y_train_ohe and y_test_ohe to create a CNN model.\n",
        "Once created get a summary, then compile and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDpu7ahTpRgB"
      },
      "source": [
        "def create_model(max_features = 1000, embedding_dims = 50,  maxlen = 400):\n",
        "    #Firstly, instantiate a sequential model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "    model.add(Conv1D(256, 3, padding='valid', activation='relu', strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = create_model(maxlen = maxlen)"
      ],
      "metadata": {
        "id": "asrZB-Jt0b9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.summary()"
      ],
      "metadata": {
        "id": "hUtIV_LO0gV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(loss = 'binary_crossentropy',  optimizer ='adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "ypPcp7j20l0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn.fit(X_train, y_train_ohe,\n",
        "        #The identify the batch size\n",
        "        batch_size = batch_size,\n",
        "        #Specify your epochs\n",
        "        epochs = 10,\n",
        "        #And identify your test data (remember to put brackets around both x_test and y_test)\n",
        "        validation_data = (X_test, y_test_ohe))"
      ],
      "metadata": {
        "id": "HgrP6oV80uc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cnn.evaluate(X_test, y_test_ohe, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(score[1]))"
      ],
      "metadata": {
        "id": "_pQ8xbwQ06my"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}